{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import interactive\n",
    "interactive(True)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, LSTM\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import History"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import TradingGym\n",
    "ENV_NAME = 'trading-v0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Trading Days into Train-Val-Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf_path = '../../Data/Si-3.18/hdf5/Si-3_18.h5'\n",
    "keys = []\n",
    "with pd.HDFStore(hdf_path) as store:\n",
    "    for key in store:\n",
    "        keys.append(key)\n",
    "trading_days = len(keys)\n",
    "print(\"Trading days: %d\" % trading_days)\n",
    "\n",
    "\n",
    "sz = {\n",
    "    'train' : int(0.4 * trading_days),\n",
    "    'val' : int(0.3 * trading_days),\n",
    "    'test' : trading_days - (int(0.4 * trading_days) + int(0.3 * trading_days)),\n",
    "}\n",
    "\n",
    "np.random.seed(123)\n",
    "indexes = np.random.permutation(trading_days)\n",
    "ids = {\n",
    "    'train' : indexes[:sz['train']],\n",
    "    'val' : indexes[sz['train']:sz['train']+sz['val']],\n",
    "    'test' : indexes[sz['train']+sz['val']:],\n",
    "}\n",
    "assert(sz['train'] == len(ids['train']))\n",
    "assert(sz['val'] == len(ids['val']))\n",
    "assert(sz['test'] == len(ids['test']))\n",
    "print(\"Train size: %d\" % sz['train'])\n",
    "print(\"Validation size: %d\" % sz['val'])\n",
    "print(\"Test size: %d\" % sz['test'])\n",
    "\n",
    "splits = {\n",
    "    'train' : [keys[i] for i in ids['train']],\n",
    "    'val' : [keys[i] for i in ids['val']],\n",
    "    'test' : [keys[i] for i in ids['test']],\n",
    "}\n",
    "\n",
    "def sample(split):\n",
    "    return splits[split][np.random.randint(0, sz[split])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = 'trading-v0'\n",
    "def makeRandomEnv(split):\n",
    "    env = gym.make(ENV_NAME)\n",
    "    key = sample(split)\n",
    "    print(\"%s: %s\" % (split, key))\n",
    "    env.init(hdf_path, key)\n",
    "    return env\n",
    "\n",
    "def iterateEnv(split):\n",
    "    env = gym.make(ENV_NAME)\n",
    "    for key in splits[split]:\n",
    "        env.init(hdf_path, key)\n",
    "        yield env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_env = gym.make(ENV_NAME)\n",
    "nb_actions = model_env.ACTION_SPACE\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + model_env.observation_space.shape))\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('elu'))\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('elu'))\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('elu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.policy import GreedyQPolicy\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.agents.cem import CEMAgent\n",
    "from rl.memory import EpisodeParameterMemory\n",
    "from rl.agents import SARSAAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "\n",
    "NB_STEPS_WARMUP = 1000\n",
    "MEM_LIMIT = 10\n",
    "LR = 1e-4\n",
    "TARGET_MODEL_UPDATE = 1e-3\n",
    "WINDOW_LENGTH = 1\n",
    "\n",
    "def CompileAgent(algo):\n",
    "    if (algo == 'DQN'):\n",
    "        memory = SequentialMemory(limit=MEM_LIMIT, window_length=WINDOW_LENGTH)\n",
    "        policy = BoltzmannQPolicy()\n",
    "        test_policy = GreedyQPolicy()\n",
    "        dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=NB_STEPS_WARMUP,\n",
    "               target_model_update=TARGET_MODEL_UPDATE, policy=policy, test_policy = GreedyQPolicy())\n",
    "        dqn.compile(Adam(lr=LR), metrics=['mae'])\n",
    "        \n",
    "        return dqn\n",
    "    elif (algo == 'CEM'):\n",
    "        memory = EpisodeParameterMemory(limit=MEM_LIMIT, window_length=WINDOW_LENGTH)\n",
    "        cem = CEMAgent(model=model, nb_actions=nb_actions, memory=memory,\n",
    "                       batch_size=10, nb_steps_warmup=NB_STEPS_WARMUP, train_interval=10,\n",
    "                       elite_frac=0.20, noise_decay_const=0.1, noise_ampl=1.0)\n",
    "        cem.compile()\n",
    "        \n",
    "        return cem\n",
    "    elif (algo == 'DNDQN'):\n",
    "        memory = SequentialMemory(limit=MEM_LIMIT, window_length=WINDOW_LENGTH)\n",
    "        policy = BoltzmannQPolicy()\n",
    "        test_policy = GreedyQPolicy()\n",
    "        # enable the dueling network\n",
    "        # you can specify the dueling_type to one of {'avg','max','naive'}\n",
    "        dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=NB_STEPS_WARMUP,\n",
    "                       enable_dueling_network=True, dueling_type='avg', target_model_update=TARGET_MODEL_UPDATE,\n",
    "                       policy=policy, test_policy = GreedyQPolicy())\n",
    "        dqn.compile(Adam(lr=LR), metrics=['mae'])\n",
    "        \n",
    "        return dqn\n",
    "    elif (algo == 'SARSA'):\n",
    "        # SARSA does not require a memory.\n",
    "        policy = BoltzmannQPolicy()\n",
    "        test_policy = GreedyQPolicy()\n",
    "        sarsa = SARSAAgent(model=model, nb_actions=nb_actions, nb_steps_warmup=WINDOW_LENGTH,\n",
    "                           policy=policy, test_policy = GreedyQPolicy())\n",
    "        sarsa.compile(Adam(lr=LR), metrics=['mae'])\n",
    "        \n",
    "        return sarsa\n",
    "    else:\n",
    "        raise NameError('Unknown RL algorithm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Val Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(trainEpisodes, testEpisodes):\n",
    "    df_all = pd.DataFrame()\n",
    "    ALGO_NAMES = ['DQN', 'CEM', 'DNDQN', 'SARSA']\n",
    "    \n",
    "    for algo in ALGO_NAMES:\n",
    "        print('Training algo {}'.format(algo))\n",
    "        agent = CompileAgent(algo)\n",
    "        for env in iterateEnv('train'):\n",
    "                agent.fit(env, nb_steps=env.EPISODE*trainEpisodes, visualize=False, verbose=False)\n",
    "               \n",
    "        print('Validating algo {}'.format(algo))\n",
    "        agent.training = False\n",
    "        df_new = []\n",
    "        history = History()\n",
    "        for env in iterateEnv('val'):\n",
    "            agent.test(env, nb_episodes=testEpisodes, callbacks=[history], verbose=False)\n",
    "            df_new += history.history['episode_reward']\n",
    "        df_all[algo] = df_new\n",
    "        \n",
    "        mean = np.mean(np.array(df_new))\n",
    "        std = np.std(np.array(df_new))\n",
    "        z = mean / std\n",
    "        print(\"mu: %.1f, sigma: %.1f, z: %.1f\" % (mean, std, z))\n",
    "        \n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = run(10, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "sns.violinplot(data=df*1e-3, ax=ax)\n",
    "ax.set_xlabel('Algorithm')\n",
    "ax.set_ylabel('Return in Rubles')\n",
    "plt.show()\n",
    "fig.savefig('violin.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
